# InferActive Server Configuration
# Copy this file to .env and customize the values for your setup

# =============================================================================
# MODEL CONFIGURATION (REQUIRED)
# =============================================================================

# Path to your model directory (must contain config.json, tokenizer files, etc.)
INFERACTIVE_MODEL_PATH=/backend/models/llama-3.1-8b-instruct_fp16

# Model type - determines chat template and special token handling
# Options: llama, qwen, exaone
INFERACTIVE_MODEL_TYPE=llama

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Host to bind the server to
# Use 0.0.0.0 to accept connections from any IP, 127.0.0.1 for localhost only
INFERACTIVE_HOST=0.0.0.0

# Port for the server to listen on
INFERACTIVE_PORT=8008

# WebSocket URL for frontend connection
# Format: ws://hostname:port/ws or wss://hostname:port/ws for SSL
REACT_APP_WS_URL=ws://localhost:8008/ws

# =============================================================================
# GPU CONFIGURATION
# =============================================================================

# GPU device ID to use for inference
# Use 0 for first GPU, 1 for second GPU, etc.
# This sets CUDA_VISIBLE_DEVICES internally
INFERACTIVE_GPU_ID=0

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

# Logging level - controls verbosity of server output
# Options: DEBUG, INFO, WARNING, ERROR
# DEBUG: Very verbose, shows all operations
# INFO: Normal operation logs
# WARNING: Only warnings and errors
# ERROR: Only error messages
INFERACTIVE_LOG_LEVEL=INFO

# =============================================================================
# MODEL MANAGEMENT
# =============================================================================

# Time in seconds before automatically unloading model due to inactivity
# Higher values = better responsiveness but more GPU memory usage
# Lower values = lower memory usage but slower response after idle period
# Set to 0 to disable automatic unloading
INFERACTIVE_MODEL_UNLOAD_TIMEOUT=600

# =============================================================================
# PERFORMANCE CONFIGURATION
# =============================================================================

# Maximum batch size for inference requests
# Higher values = better throughput but more GPU memory usage
# Adjust based on your GPU memory capacity
# Recommended: 8-16 for RTX 3080/4090, 16-32 for A100/H100
INFERACTIVE_BATCH_SIZE=16

# Batch timeout in seconds - how long to wait collecting requests before processing
# Lower values = lower latency but potentially less efficient batching
# Higher values = better throughput but higher latency
# Recommended: 0.05-0.2 seconds
INFERACTIVE_BATCH_TIMEOUT=0.1

# =============================================================================
# CORS CONFIGURATION
# =============================================================================

# CORS allowed origins for web browser access
# Use ["*"] to allow all origins (development only)
# Use specific origins for production: ["https://yourdomain.com", "https://app.yourdomain.com"]
INFERACTIVE_CORS_ORIGINS=["*"]

# =============================================================================
# EXAMPLE CONFIGURATIONS FOR DIFFERENT SETUPS
# =============================================================================

# High-Performance Setup (A100/H100 GPUs):
# INFERACTIVE_BATCH_SIZE=32
# INFERACTIVE_BATCH_TIMEOUT=0.05
# INFERACTIVE_MODEL_UNLOAD_TIMEOUT=3600


# =============================================================================
# USAGE EXAMPLES
# =============================================================================

# After copying this file to .env and setting your values, you can:
#
# 1. Run with environment configuration:
#    inferactive-server
#
# 2. Override specific settings via CLI:
#    inferactive-server --port 8008 --log-level debug
#
# 3. Check your configuration:
#    curl http://localhost:8008/stats